{
  "name": "Febei",
  "tagline": "Face Expression Based Emoticon Identification",
  "body": "# Face Expression Based Emoticon Identification\r\n\r\nThe Face Expression Based Emoticon Identification (FEBEI) system is an open source extension to the Tracker.js framework which converts a human facial expression to the best matching emoticon. The contribution of this project is to build this robust classifier which can identify facial expression in real time without any reliance on an external server or computation node. An entirely client-side JavaScript implementation has clear privacy benefits as well as the avoidance of any lag inherent in uploading and downloading images. We accomplished this by utilizing several computationally efficient methods. Tracking.js provided a Viola Jones based face detector which we used to pass facial images to our own implementation of an eigenemotion detection system which is trained to distinguish between happy and angry faces.\r\n\r\n# How to use\r\n\r\nThe folder Eigenemotions contains the source code which exists as tracking-extension.js. There is also a sample implementation of the eigenemotions. This priliminary version can detect 2 emotions - angry and happy.\r\n\r\nInvoking the tracking extension is simple. For a static image it can be invoked as follows:\r\n\r\n\t\tvar img = document.getElementById(<image element selector>);\r\n\t\tvar tracker = new tracking.ObjectTracker('face');\r\n\t\ttracker.setStepSize(1.7);\r\n\t\ttracking.track('#img', tracker);\r\n\t\tvar canvas = document.createElement('canvas');\r\n\t\tvar context = canvas.getContext('2d');\r\n\t\tcontext.drawImage(img, 0, 0 );\r\n\t\ttracking.detect('EigenEmotions', '#img', context, canvas, tracker, {});\r\n\r\n\r\nThe below is the code for invoking tracking extension for face expression recognition on live feed video:\r\n\r\n\t\tvar video = document.getElementById(<selector for video tag>);\r\n\t\tvar canvas = document.getElementById(<selector for canvas tag>);\r\n\t\tvar context = canvas.getContext('2d');\r\n\t\tvar tracker = new tracking.ObjectTracker('face');\r\n\t\ttracker.setInitialScale(4);\r\n\t\ttracker.setStepSize(2);\r\n\t\ttracker.setEdgesDensity(0.1);\r\n\t\ttracking.detect('EigenEmotions', '#video', context, canvas, tracker, { camera: true });\r\n\r\nBoth of these are implemented in an example in face_exp.html\r\n\r\nThe library has to be included in you project with multiple files. The typical set of files you would include are:\r\n\r\n\t\t<script src=\"build/tracking.js\"></script>\r\n\t\t<script src=\"build/data/face-min.js\"></script>\r\n\t\t<script src=\"assets/stats.min.js\"></script>\r\n\t\t<script src=\"numeric.js\"></script>\r\n\t\t<script src=\"svm.js\"></script>\r\n\t\t<script src=\"build/data/svm_model.js\"></script>\r\n\t\t<script src=\"build/data/pca_data.js\"></script>\r\n\t\t<script src=\"build/data/pca_labels.js\"></script>\r\n\t\t<script src=\"build/data/pca_mean.js\"></script>\r\n\t\t<script src=\"build/data/pca_components.js\"></script>\r\n\t\t<script src=\"build/tracking_extension.js\"></script>\r\n\t\t<script src=\"https://code.jquery.com/jquery-2.2.3.min.js\"></script>\r\n\r\n\r\nAny version of jquery can be included. The contents of the file are pretty self explanatory. Most of the JS files contains the raw model exported from the training on python. Singular Vector Decomposition (SVD) is achieved by importing the numeric.js library by SÃ©bastien Loisel from http://www.numericjs.com/. The SVM is obtained from the open sourced library svmjs by Andrej Karpathy from https://github.com/karpathy/svmjs. Tracking.js serves as a backbone for this system and it can be found at https://trackingjs.com/. \r\n\r\n# How to train\r\n\r\nThe training of this model can seem a bit intimidating but is actually not so. These are the steps involved:\r\n\r\n# 1. Changing the dataset\r\n\r\nThe folder emotions contains the actual images. Just change or add images to that. The subfolders are the classes. Pretty straightforward, no twists here. But it doesn't end there. You need to compress the new dataset into face_expressions.tgz and create new devtest and devtrain pair files. These are of the following format:\r\n\r\n\t\t10\r\n\t\thappyfaces S026_006_00000006 S050_006_00000013\r\n\t\thappyfaces S075_006_00000015 S075_006_00000011\r\n\t\thappyfaces S057_006_00000019 S075_006_00000023\r\n\t\thappyfaces S109_006_00000012 S076_006_00000007\r\n\t\thappyfaces S026_006_00000006 angryfaces S042_005_00000019\r\n\r\nThe first line contains the number of cases and the lines after that are cases. Each line is of format:\r\n\r\n\t\t<class> <file1> {<different_class>} <file2>\r\n\r\nthe PairsDevTest.txt is the test file and PairsDevTrain.txt is the train file. Make sure you give as many training samples with as many combinations as possible.\r\n\r\nThe last step is that now copy the whole face-rec folder (with the updated dataset and pair files) and place it as it is in your local server. I used the Apache server (MAMP, XAMPP, LAMP whichever u prefer) and just placed it in the htdocs folder. I would recommend the same for all to avoid hassles (You can contact us in any case, we will do the best to help).\r\n\r\n# 2. Training the new model\r\n\r\nWe have added a new model to scikit learn for the dataset. This is the face_expressions.py. This will download the face library that we have put to htdocs, resize and get the right frames for training and classification. This file should be placed in:\r\n\r\n\t\t/Library/Python/2.7/site-packages/sklearn/datasets\r\n\r\nThis is now usable to train. The file you should run to train and test is the face_rec.py in the python-face-rec-library folder. Once you change the dataset, ready the sklearn and run this you will obtain the following things:\r\n\r\n1. A PCA model pickle file\r\n2. A SVM pickle file\r\n3. A model.txt file with the training vectors\r\n4. A model_label.txt file with the training labels\r\n5. An example classification result\r\n6. The Eigenface pictures\r\n\r\nThe first two are for the evaluatory python server inputs and do not need to be touched. The rest 2 are for the JS. The data in the model.txt should replace the data/pca_data.js file and the model_label.txt should replace the data/pca_labels.js.\r\n\r\nCongrats!! You have completed the first phase of training the new model.\r\n\r\nThe next phase is fairly simple. Just visit the svm_train.html in your server with all the files present. It will train the new pca data and print the new numeric model which you should copy and replace the existing model in data/svm_model.js.\r\n\r\nAnd hence you have trained a new model. Good job!!\r\n\r\n# 3. Evaluating the new model\r\n\r\nOne can evaluate the model using the python flask server based classification. The code for this is also inherently present. The python server is face_server.py just do:\r\n\r\n\t\tpython face_server.py\r\n\r\nand the server should with the newest model loaded.\r\n\r\n# Experiment with freedom\r\n\r\nWe are a bunch of open source enthusiasts crazy about AI and vision. This is an ongoing project and the modules that we have made to work are powerful and promising. Hence we would like you to change the code to your needs and weave innovative uses out of it in your own way....no frills.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}